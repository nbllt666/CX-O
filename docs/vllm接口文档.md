### ğŸŒ APIè°ƒç”¨æŒ‡å—ï¼ˆå«æµå¼è¾“å‡ºï¼‰

vLLMæœåŠ¡å¯åŠ¨åï¼Œä¼šæä¾›ä¸€ä¸ªä¸OpenAI APIå…¼å®¹çš„æ¥å£ï¼ŒåŸºç¡€URLé€šå¸¸æ˜¯ `http://localhost:8000/v1`ï¼ˆå¦‚æœä½ æ˜ å°„äº†å…¶ä»–ç«¯å£ï¼Œå¦‚8010ï¼Œåˆ™éœ€ç›¸åº”æ›´æ”¹ï¼‰ã€‚

#### æ™®é€šéæµå¼è°ƒç”¨
ä½¿ç”¨Pythonå®¢æˆ·ç«¯è¿›è¡Œç®€å•çš„èŠå¤©è¡¥å…¨è°ƒç”¨ç¤ºä¾‹ ï¼š
```python
from openai import OpenAI

client = OpenAI(
    base_url="http://localhost:8000/v1", # å¦‚æœç«¯å£æ˜ å°„ä¸º8010ï¼Œåˆ™æ”¹ä¸º8010
    api_key="EMPTY"  # vLLMé»˜è®¤ä¸éœ€è¦é‰´æƒï¼Œä½†æŸäº›ç‰ˆæœ¬éœ€è¦éšä¾¿å¡«ä¸ªkey
)

completion = client.chat.completions.create(
    model="Qwen2.5-7B-Instruct", # ä¸ --served-model-name ä¸€è‡´
    messages=[
        {"role": "user", "content": "è¯·ç”¨ä¸€å¥è¯ä»‹ç»äººå·¥æ™ºèƒ½ã€‚"}
    ],
    max_tokens=100,
    temperature=0.7
)
print(completion.choices[0].message.content)
```

#### å®ç°æµå¼è¾“å‡º
æµå¼è¾“å‡ºå¯¹äºå®ç°ç±»ä¼¼ChatGPTçš„æ‰“å­—æœºæ•ˆæœè‡³å…³é‡è¦ï¼Œå®ƒèƒ½æ˜¾è‘—æå‡ç”¨æˆ·ä½“éªŒã€‚vLLMåŸç”Ÿæ”¯æŒæ­¤åŠŸèƒ½ ã€‚

åœ¨Pythonä¸­ï¼Œä½ åªéœ€è¦åœ¨è°ƒç”¨æ—¶è®¾ç½® `stream=True`ï¼Œç„¶åè¿­ä»£è¿”å›çš„å“åº”å³å¯ ï¼š
```python
from openai import OpenAI

client = OpenAI(base_url="http://localhost:8000/v1", api_key="EMPTY")

stream = client.chat.completions.create(
    model="Qwen2.5-7B-Instruct",
    messages=[
        {"role": "user", "content": "ç»™æˆ‘è®²ä¸€ä¸ªå…³äºåšæŒä¸æ‡ˆçš„æ•…äº‹ã€‚"}
    ],
    max_tokens=500,
    temperature=0.7,
    stream=True  # å¯ç”¨æµå¼è¾“å‡º
)

for chunk in stream:
    # æ£€æŸ¥æ˜¯å¦æœ‰æ–°çš„å†…å®¹å¢é‡
    if chunk.choices[0].delta.content is not None:
        print(chunk.choices[0].delta.content, end="", flush=True) # é€å—æ‰“å°
```
**æµå¼è¾“å‡ºçš„æ ¸å¿ƒåŸç†**æ˜¯æœåŠ¡å™¨ç«¯æ¯ç”Ÿæˆä¸€å°æ®µTokenï¼ˆä¾‹å¦‚4ä¸ªï¼‰å°±ç«‹å³å‘é€ç»™å®¢æˆ·ç«¯ï¼Œè€Œä¸æ˜¯ç­‰å¾…æ•´ä¸ªå“åº”å®Œæˆ ã€‚è¿™ä¾èµ–äºServer-Sent Events (SSE) åè®® ã€‚

#### å¤šæ¨¡æ€è°ƒç”¨ç¤ºä¾‹
å¯¹äºæ”¯æŒå¤šæ¨¡æ€çš„æ¨¡å‹ï¼ŒAPIè°ƒç”¨æ–¹å¼ç±»ä¼¼ï¼Œä½†æ¶ˆæ¯ä½“ä¸­çš„ `content` å¯ä»¥æ˜¯ä¸€ä¸ªåŒ…å«å›¾åƒå’Œæ–‡æœ¬çš„åˆ—è¡¨ã€‚å›¾åƒé€šå¸¸éœ€è¦ä»¥Base64ç¼–ç æ ¼å¼ä¼ å…¥ ã€‚
```python
# æ³¨æ„ï¼šè¿™æ˜¯ä¸€ä¸ªç®€åŒ–çš„ç¤ºä¾‹ï¼Œå®é™…ä½¿ç”¨æ—¶éœ€è¦å…ˆè¿›è¡Œå›¾åƒç¼–ç ç­‰æ“ä½œ
completion = client.chat.completions.create(
    model="Qwen2-VL-7B-Instruct",
    messages=[
        {
            "role": "user",
            "content": [
                {"type": "image_url", "image_url": {"url": "data:image/jpeg;base64,..."}}, # æ›¿æ¢ä¸ºä½ çš„å›¾ç‰‡base64æ•°æ®
                {"type": "text", "text": "æè¿°ä¸€ä¸‹è¿™å¼ å›¾ç‰‡ã€‚"}
            ]
        }
    ],
    max_tokens=300
)
```


ğŸ› ï¸ å·¥å…·è°ƒç”¨ï¼ˆTool Callingï¼‰

vLLM æ”¯æŒä¸ OpenAI API å…¼å®¹çš„å·¥å…·è°ƒç”¨åŠŸèƒ½ï¼Œè®©æ‚¨èƒ½å¤Ÿå®šä¹‰å‡½æ•°ä¾›å¤§è¯­è¨€æ¨¡å‹åœ¨å¯¹è¯è¿‡ç¨‹ä¸­è°ƒç”¨ã€‚è¿™ä¸€åŠŸèƒ½å¯¹äºæ„å»ºèƒ½å¤Ÿä¸å¤–éƒ¨ç³»ç»Ÿäº¤äº’çš„æ™ºèƒ½åº”ç”¨è‡³å…³é‡è¦ã€‚
ğŸš€ å¿«é€Ÿå…¥é—¨
æœåŠ¡ç«¯é…ç½®

è¦å¯ç”¨å·¥å…·è°ƒç”¨åŠŸèƒ½ï¼Œéœ€åœ¨å¯åŠ¨ vLLM æœåŠ¡æ—¶æ·»åŠ ç‰¹å®šå‚æ•°ã€‚ä»¥ä¸‹æ˜¯ä½¿ç”¨ Llama 3.1 æ¨¡å‹çš„ç¤ºä¾‹ï¼š

bash
vllm serve meta-llama/Llama-3.1-8B-Instruct \
--enable-auto-tool-choice \
--tool-call-parser llama3_json \
--chat-template examples/tool_chat_template_llama3.1_json.jinja
å®¢æˆ·ç«¯è°ƒç”¨ç¤ºä¾‹

python
from openai import OpenAI
import json

client = OpenAI(base_url="http://localhost:8000/v1", api_key="EMPTY")
å®šä¹‰å·¥å…·å‡½æ•°
def get_weather(location: str, unit: str):
return f"æ­£åœ¨è·å– {location} çš„å¤©æ°”ï¼Œå•ä½ä¸º {unit}..."
å®šä¹‰å·¥å…· schema
tools = [{
"type": "function",
"function": {
"name": "get_weather",
"description": "è·å–æŒ‡å®šåœ°ç‚¹çš„å½“å‰å¤©æ°”",
"parameters": {
"type": "object",
"properties": {
"location": {"type": "string", "description": "åŸå¸‚å’Œå·ï¼Œä¾‹å¦‚ 'San Francisco, CA'"},
"unit": {"type": "string", "enum": ["celsius", "fahrenheit"]}
},
"required": ["location", "unit"]
}
}
}]
å‘é€è¯·æ±‚
response = client.chat.completions.create(
model="meta-llama/Llama-3.1-8B-Instruct",
messages=[{"role": "user", "content": "æ—§é‡‘å±±çš„å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ"}],
tools=tools,
tool_choice="auto" # å¯é€‰: "auto", "none" æˆ–æŒ‡å®šç‰¹å®šå‡½æ•°
)
å¤„ç†å·¥å…·è°ƒç”¨
if response.choices[0].message.tool_calls:
tool_call = response.choices[0].message.tool_calls[0].function
print(f"è°ƒç”¨çš„å‡½æ•°: {tool_call.name}")
print(f"å‚æ•°: {tool_call.arguments}")

# æ‰§è¡Œå‡½æ•°å¹¶è·å–ç»“æœ
args = json.loads(tool_call.arguments)
result = get_weather(*args)
print(f"ç»“æœ: {result}")

# å°†ç»“æœè¿”å›ç»™æ¨¡å‹ï¼ˆå®Œæ•´å¯¹è¯æµç¨‹ï¼‰
messages = [
{"role": "user", "content": "æ—§é‡‘å±±çš„å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ"},
{
"role": "assistant",
"tool_calls": [{
"id": response.choices[0].message.tool_calls[0].id,
"type": "function",
"function": {
"name": tool_call.name,
"arguments": tool_call.arguments
}
}]
},
{
"role": "tool",
"tool_call_id": response.choices[0].message.tool_calls[0].id,
"content": result
}
]

# è·å–æœ€ç»ˆå›å¤
final_response = client.chat.completions.create(
model="meta-llama/Llama-3.1-8B-Instruct",
messages=messages
)
print(f"æœ€ç»ˆå›å¤: {final_response.choices[0].message.content}")
ğŸ”§ æ”¯æŒçš„æ¨¡å‹ä¸é…ç½®

vLLM ä¸ºä¸åŒæ¨¡å‹æä¾›ä¸“é—¨çš„å·¥å…·è°ƒç”¨è§£æå™¨ï¼Œä»¥ä¸‹æ˜¯å¸¸ç”¨é…ç½®ï¼š
Llama 3.1 ç³»åˆ—
bash
--tool-call-parser llama3_json \
--chat-template examples/tool_chat_template_llama3_json.jinja

æ”¯æŒæ¨¡å‹ï¼šmeta-llama/Meta-Llama-3.1-8B/70B/405B-Instruct
Hermes ç³»åˆ—
bash
--tool-call-parser hermes

æ”¯æŒæ¨¡å‹ï¼šNousResearch/Hermes-2-Pro-, NousResearch/Hermes-3-*
Mistral ç³»åˆ—
bash
--tool-call-parser mistral \
--chat-template examples/tool_chat_template_mistral_parallel.jinja

æ”¯æŒæ¨¡å‹ï¼šmistralai/Mistral-7B-Instruct-v0.3 ç­‰
Python é£æ ¼å·¥å…·è°ƒç”¨
bash
--tool-call-parser pythonic \
--chat-template examples/tool_chat_template_llama3.2_pythonic.jinja

æ”¯æŒæ¨¡å‹ï¼šmeta-llama/Llama-3.2-1B/3B-Instruct, Team-ACE/ToolACE-8B
ğŸ¤– é«˜çº§ç”¨æ³•
æŒ‡å®šç‰¹å®šå·¥å…·

é™¤äº† tool_choice="auto"ï¼Œæ‚¨è¿˜å¯ä»¥å¼ºåˆ¶æ¨¡å‹ä½¿ç”¨ç‰¹å®šå·¥å…·ï¼š

python
tool_choice = {
"type": "function",
"function": {"name": "get_weather"}
}

response = client.chat.completions.create(
# å…¶ä»–å‚æ•°ä¸å˜...
tool_choice=tool_choice
)
æµå¼å·¥å…·è°ƒç”¨

æµå¼å“åº”åŒæ ·æ”¯æŒå·¥å…·è°ƒç”¨ï¼Œå¤„ç†æ–¹å¼ç•¥æœ‰ä¸åŒï¼š

python
stream = client.chat.completions.create(
# å‚æ•°åŒä¸Š...
stream=True
)

full_response = ""
for chunk in stream:
if chunk.choices[0].delta.content:
print(chunk.choices[0].delta.content, end="", flush=True)
full_response += chunk.choices[0].delta.content
if chunk.choices[0].delta.tool_calls:
# å¤„ç†æµå¼å·¥å…·è°ƒç”¨
print("\n[æ£€æµ‹åˆ°å·¥å…·è°ƒç”¨]", flush=True)
è‡ªå®šä¹‰å·¥å…·è§£æå™¨

å¯¹äºä¸æ”¯æŒçš„æ¨¡å‹ï¼Œæ‚¨å¯ä»¥å¼€å‘è‡ªå®šä¹‰å·¥å…·è§£æå™¨æ’ä»¶ï¼š

1. åˆ›å»ºæ’ä»¶æ–‡ä»¶ my_tool_parser.py:
python
from vllm.entrypoints.openai.tool_parsers import ToolParser
from vllm.entrypoints.openai.tool_parsers import ToolParserManager

@ToolParserManager.register_module(["my_parser"])
class MyCustomToolParser(ToolParser):
# å®ç°å¿…è¦çš„æ–¹æ³•
pass

2. å¯åŠ¨æœåŠ¡æ—¶æŒ‡å®šæ’ä»¶ï¼š
bash
--enable-auto-tool-choice \
--tool-parser-plugin /path/to/my_tool_parser.py \
--tool-call-parser my_parser \
--chat-template /path/to/chat_template.jinja
âš ï¸ æ³¨æ„äº‹é¡¹

1. å·¥å…·è°ƒç”¨æµç¨‹ï¼šå·¥å…·è°ƒç”¨é€šå¸¸éœ€è¦å¤šè½®å¯¹è¯å®Œæˆï¼ŒåŒ…æ‹¬å·¥å…·è°ƒç”¨è¯·æ±‚ã€å·¥å…·æ‰§è¡Œå’Œç»“æœè¿”å›ã€‚
2. å‘½åé™åˆ¶ï¼šå‡½æ•°ååº”ä½¿ç”¨å°å†™å­—æ¯å’Œä¸‹åˆ’çº¿ï¼Œé¿å…ç‰¹æ®Šå­—ç¬¦ã€‚
3. å‚æ•°éªŒè¯ï¼šå§‹ç»ˆéªŒè¯æ¨¡å‹è¿”å›çš„å‚æ•°ï¼Œé¿å…æ³¨å…¥æ”»å‡»ã€‚
4. æ¨¡å‹èƒ½åŠ›ï¼šè¾ƒå°çš„æ¨¡å‹ï¼ˆå¦‚7Bçº§åˆ«ï¼‰åœ¨å¤æ‚å·¥å…·è°ƒç”¨åœºæ™¯ä¸­å¯èƒ½è¡¨ç°ä¸ä½³ã€‚
5. æ€§èƒ½å½±å“ï¼šé¦–æ¬¡ä½¿ç”¨å‘½åå‡½æ•°è°ƒç”¨æ—¶ä¼šæœ‰å»¶è¿Ÿï¼Œå› ä¸ºå¼•å¯¼å¼è§£ç åç«¯éœ€è¦ç¼–è¯‘æœ‰é™çŠ¶æ€æœº(FSM)ã€‚

å·¥å…·è°ƒç”¨åŠŸèƒ½ä¸ºLLMåº”ç”¨æ‰“å¼€äº†ä¸å¤–éƒ¨ä¸–ç•Œäº¤äº’çš„å¤§é—¨ï¼Œæ­£ç¡®ä½¿ç”¨å®ƒå¯ä»¥è®©æ‚¨çš„åº”ç”¨æ›´åŠ æ™ºèƒ½å’Œå®ç”¨ï¼
