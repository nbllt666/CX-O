晨曦Origins人格化AI助手长期记忆系统（CX-O-HMS）设计文档

1. 系统概述

1.1 设计目标

本长期记忆系统旨在为“人格化”AI助手构建一个兼具仿生记忆特性与数据持久性的核心模块。通过主副模型协作、工具调用和RAG技术，使助手能够形成连续、安全、可控的长期记忆，从而从“功能可用”走向“情感可信”。

1.2 核心特性

•   分层记忆模型：区分永久记忆与长期记忆，并辅以自动归档机制。

•   主副模型分工：主模型（Master LLM）主导交互与决策，副模型（Assistant LLM）专职管理记忆生命周期。

•   工具驱动协作：通过定义明确的工具集（Tools）实现主副模型间安全、可控的指令传递与数据操作。

•   用户主权与安全：永久记忆仅用户可删，长期记忆删除需用户确认，确保记忆的最终控制权归属用户。

•   通用性设计：系统架构解耦，可适配于个人AI助手、AI直播系统等多种场景。

2. 系统架构设计

2.1 核心组件

组件模块 核心职责 关键技术/机制

主模型 主导与用户的交互、复杂推理、任务规划，并决定何时调用记忆系统。 工具调用能力

副模型 记忆系统的专职管理员。执行记忆的摘要、归档、检索和清理。 专用的记忆管理工具集、上下文摘要能力

记忆存储器 持久化存储记忆数据。包括永久记忆库、长期记忆库（含归档分区）。 向量数据库（用于语义检索）、关系型数据库（用于元数据管理）

记忆路由器 处理所有记忆检索请求，调度RAG流程，融合多源记忆。 向量相似度检索、元数据过滤、检索结果重排

2.2 记忆数据模型与流程

系统的核心工作流程围绕记忆的写入与读取展开，下图清晰地展示了信息在不同存储层级间的流动与转化逻辑：
flowchart TD
    A[用户输入/对话上下文] --> B[记忆路由器]
    B --> C{检索判断}
    C -- 用户查询/主模型主动检索 --> D[RAG检索<br>永久记忆库 & 长期记忆库]
    D --> E[生成增强上下文]
    E --> F[主模型生成回复]

    A --> G[短期记忆缓冲池]
    G --> H{归档触发?}
    H -- 定时触发/主模型指令 --> I[副模型]
    I --> J[摘要与整合]
    J --> K[写入长期记忆库<br>对应时间分区]
    
    F --> L[主模型判断为关键信息]
    L -- 调用工具 --> M[写入永久记忆库]


3. 核心模块详述

3.1 记忆的分层与归档策略

记忆类型 存储内容 管理方式与生命周期

永久记忆 用户的核心身份认同、极端偏好、绝对规则（如“对坚果严重过敏”）。 由主模型通过专用工具（如save_permanent_memory）直接写入。仅支持用户直接删除。

长期记忆 经副模型摘要整理的、带有时序标签的对话和事件记忆。 按时间（日/周/月/年）分区存储。副模型在归档时有权修改或合并旧记忆，实现“记忆优化”。删除需用户向副模型发出指令并确认。

归档机制 长期记忆的整理过程。 副模型根据设定周期（每日/周/月），对短期记忆进行摘要，并存入长期记忆库的相应分区，形成“仿生遗忘”效应（远期记忆需更强相关才被召回）。

3.2 主副模型协作机制

•   主模型工具集：

    ◦   call_assistant(instruction: str): 呼出副模型并对其下达指令，如“总结过去两小时对话中提到的项目需求要点，并归档”。

    ◦   save_permanent_memory(content: str, reason: str): 申请写入永久记忆。

    ◦   search_memories(query: str): 主动检索记忆。

•   副模型工具集：

    ◦   create_daily_summary(context: str): 对指定上下文生成摘要。

    ◦   archive_memories(period: str): 执行指定周期的归档。

    ◦   update_memory_node(memory_id: str, new_content: str): 修改已有的长期记忆。

    ◦   search_memories(query: str, time_range: str): 检索记忆。

    ◦   delete_memory(memory_id: str, reason: str): 执行删除操作前，必须触发用户确认流程。

3.3 RAG系统与“仿生遗忘”

•   RAG流程：记忆路由器处理查询时，会并行搜索永久记忆库和长期记忆库（可指定时间范围）。采用混合检索（关键词+向量语义）并结合重排模型，确保召回结果既相关又新鲜。

•   “仿生遗忘”：通过检索策略模拟。系统配置为更偏好近期和高频记忆。随着时间推移，早期记忆除非在语义上高度相关，否则在检索中的排名会降低，不会被主动注入上下文，从而模拟人类的记忆衰减。

3.4 权限与安全控制

此设计严格遵循“用户主权”原则：
1.  永久记忆删除：唯一权限归属于用户。系统应提供直接操作界面（如命令行或GUI），绕过LLM代理，确保用户意志绝对贯彻。
2.  长期记忆删除：唯一指令发起权归属于用户。用户向副模型发出指令，副模型必须发起确认请求，用户确认后方可执行。
3.  审计日志：所有对记忆的增删改查操作都必须记录详细日志（操作者、时间、对象、来源），便于追溯和审计。
4.  建议实施软删除：所有删除操作先标记为“待删除”，保留一段时间（如7天）后再物理删除，为用户提供“后悔期”。

4. 实现指南与建议

4.1 技术栈选型参考

•   记忆存储：

    ◦   向量数据库：可用qdrate，用于存储记忆内容的向量嵌入，支持高效语义检索。

    ◦   关系型数据库：可用SQLlite，用于存储记忆元数据（ID、类型、时间戳、标签、权限信息等）。

•   工具调用协议：推荐使用Model Context Protocol，这是一种轻量级标准，非常适合在不同LLM和应用之间定义和调用工具。

•   副模型选择：可选择参数规模较小、推理速度较快的模型（如Llama 3 8B），以降低成本并提高归档效率。

4.2 关键实施步骤

1.  定义记忆数据模式：在设计阶段，AI产品经理需主导定义清晰的“记忆清单”，明确哪些信息值得存储及其分类。
2.  实现记忆存储层：建立向量库和关系库，并设计好关联索引。
3.  开发工具服务器：实现上述主副模型所需的工具集，并暴露为MCP服务器或类似的API端点。
4.  集成与测试：将主副模型连接到工具服务器，重点测试记忆的读写权限控制、归档流程的稳定性以及RAG的准确性。

4.3 直播场景适配建议

在AI直播系统中应用时：
•   记忆源：将实时弹幕、互动事件、直播片段文本作为短期记忆源。

•   归档周期：可设置更短的归档间隔（如每15分钟进行一次亮点摘要），以便主模型能快速回忆近期互动。

•   记忆触发：主模型可实时检索这些摘要，用于与观众进行深度互动，例如：“我记得刚才有位朋友提到了XX，我们来深入聊一下...”